{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a26b45-e63d-4482-b928-718f58fe1a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 15:39:00.047372: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-05 15:39:00.048774: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-05 15:39:00.074994: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-05 15:39:00.076220: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-05 15:39:00.557700: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Masking, Conv1D, Flatten, MaxPooling1D\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from synthetic_data import gen_x_y\n",
    "from get_real_data import data_augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff33a0-b6d3-489f-b2f8-65ef2b620c17",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c84eea1f-a667-4e66-8819-345723c56e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(pattern_name: str) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load data for a pattern and return them as a list of DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        pattern_names (List[str]): List of pattern names to load.\n",
    "        \n",
    "    Returns:\n",
    "        List[pd.DataFrame]: A list of DataFrames containing data for the specified patterns.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    DATA_DIR = 'data/patterns'\n",
    "    \n",
    "    dir_path = os.path.join(DATA_DIR, pattern_name)\n",
    "\n",
    "    files = os.listdir(dir_path)\n",
    "    \n",
    "    csv_files = [file for file in files if file.endswith(\".csv\")]\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(dir_path, csv_file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append(df)\n",
    "    \n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e4352d9-4ab3-4493-932a-0bcb5eaefca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>End Date</th>\n",
       "      <th>Pattern</th>\n",
       "      <th>Pattern Present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-07-18</td>\n",
       "      <td>29.8</td>\n",
       "      <td>31.6</td>\n",
       "      <td>29.8</td>\n",
       "      <td>30.5</td>\n",
       "      <td>79</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-07-19</td>\n",
       "      <td>30.8</td>\n",
       "      <td>30.9</td>\n",
       "      <td>29.9</td>\n",
       "      <td>30.5</td>\n",
       "      <td>79</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-07-20</td>\n",
       "      <td>30.9</td>\n",
       "      <td>30.9</td>\n",
       "      <td>30.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>79</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-07-21</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>30.7</td>\n",
       "      <td>79</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>30.7</td>\n",
       "      <td>31.3</td>\n",
       "      <td>30.7</td>\n",
       "      <td>31.3</td>\n",
       "      <td>79</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>30.5</td>\n",
       "      <td>30.6</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>79</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>2023-04-03</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.3</td>\n",
       "      <td>79</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>2023-04-04</td>\n",
       "      <td>30.4</td>\n",
       "      <td>30.8</td>\n",
       "      <td>30.2</td>\n",
       "      <td>30.3</td>\n",
       "      <td>79</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>2023-04-05</td>\n",
       "      <td>30.8</td>\n",
       "      <td>30.8</td>\n",
       "      <td>30.2</td>\n",
       "      <td>30.6</td>\n",
       "      <td>79</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>2023-04-06</td>\n",
       "      <td>30.6</td>\n",
       "      <td>30.6</td>\n",
       "      <td>30.2</td>\n",
       "      <td>30.2</td>\n",
       "      <td>79</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  Open  High   Low  Close  Start Date  End Date  Pattern  \\\n",
       "0    2022-07-18  29.8  31.6  29.8   30.5          79       106        1   \n",
       "1    2022-07-19  30.8  30.9  29.9   30.5          79       106        1   \n",
       "2    2022-07-20  30.9  30.9  30.6   30.7          79       106        1   \n",
       "3    2022-07-21  31.0  31.6  30.7   30.7          79       106        1   \n",
       "4    2022-07-22  30.7  31.3  30.7   31.3          79       106        1   \n",
       "..          ...   ...   ...   ...    ...         ...       ...      ...   \n",
       "179  2023-03-31  30.5  30.6  30.0   30.0          79       106        1   \n",
       "180  2023-04-03  30.0  30.4  30.0   30.3          79       106        1   \n",
       "181  2023-04-04  30.4  30.8  30.2   30.3          79       106        1   \n",
       "182  2023-04-05  30.8  30.8  30.2   30.6          79       106        1   \n",
       "183  2023-04-06  30.6  30.6  30.2   30.2          79       106        1   \n",
       "\n",
       "     Pattern Present  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  1  \n",
       "3                  1  \n",
       "4                  1  \n",
       "..               ...  \n",
       "179                1  \n",
       "180                1  \n",
       "181                1  \n",
       "182                1  \n",
       "183                1  \n",
       "\n",
       "[184 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = load_data('rising_wedge')\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7dea1b-ab87-4a96-bad2-42dc4b92b471",
   "metadata": {},
   "source": [
    "# One Hot Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3019c74f-f77d-4426-aa79-67098a485865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(dataframes: list) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    One Hot Encode data for multiple patterns and return them as a list of DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        dataframes (List[str]): List of pattern names to load.\n",
    "        \n",
    "    Returns:\n",
    "        List[pd.DataFrame]: A list of DataFrames with One Hot Encoding for the 'Pattern' column.\n",
    "    \"\"\"\n",
    "    pattern_mapping = {\n",
    "        0: 'No Pattern',\n",
    "        1: 'Rising Wedge',\n",
    "        2: 'Falling Wedge',\n",
    "        3: 'Double Top',\n",
    "        4: 'Double Bottom'\n",
    "    }\n",
    "    dataframes_encoded = []\n",
    "    \n",
    "    for df in dataframes:\n",
    "        for pattern_name in pattern_mapping.values():\n",
    "            df[pattern_name] = 0\n",
    "        \n",
    "        df[pattern_mapping[df['Pattern'].iloc[0]]] = 1\n",
    "        dataframes_encoded.append(df)\n",
    "    \n",
    "    return dataframes_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "496ce5ad-2cf8-4a0f-827a-faa34dc5a5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>End Date</th>\n",
       "      <th>Pattern</th>\n",
       "      <th>Pattern Present</th>\n",
       "      <th>No Pattern</th>\n",
       "      <th>Rising Wedge</th>\n",
       "      <th>Falling Wedge</th>\n",
       "      <th>Double Top</th>\n",
       "      <th>Double Bottom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-07-18</td>\n",
       "      <td>29.8</td>\n",
       "      <td>31.6</td>\n",
       "      <td>29.8</td>\n",
       "      <td>30.5</td>\n",
       "      <td>79</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-07-19</td>\n",
       "      <td>30.8</td>\n",
       "      <td>30.9</td>\n",
       "      <td>29.9</td>\n",
       "      <td>30.5</td>\n",
       "      <td>79</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-07-20</td>\n",
       "      <td>30.9</td>\n",
       "      <td>30.9</td>\n",
       "      <td>30.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>79</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-07-21</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>30.7</td>\n",
       "      <td>79</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>30.7</td>\n",
       "      <td>31.3</td>\n",
       "      <td>30.7</td>\n",
       "      <td>31.3</td>\n",
       "      <td>79</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>30.5</td>\n",
       "      <td>30.6</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>79</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>2023-04-03</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.3</td>\n",
       "      <td>79</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>2023-04-04</td>\n",
       "      <td>30.4</td>\n",
       "      <td>30.8</td>\n",
       "      <td>30.2</td>\n",
       "      <td>30.3</td>\n",
       "      <td>79</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>2023-04-05</td>\n",
       "      <td>30.8</td>\n",
       "      <td>30.8</td>\n",
       "      <td>30.2</td>\n",
       "      <td>30.6</td>\n",
       "      <td>79</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>2023-04-06</td>\n",
       "      <td>30.6</td>\n",
       "      <td>30.6</td>\n",
       "      <td>30.2</td>\n",
       "      <td>30.2</td>\n",
       "      <td>79</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  Open  High   Low  Close  Start Date  End Date  Pattern  \\\n",
       "0    2022-07-18  29.8  31.6  29.8   30.5          79       106        1   \n",
       "1    2022-07-19  30.8  30.9  29.9   30.5          79       106        1   \n",
       "2    2022-07-20  30.9  30.9  30.6   30.7          79       106        1   \n",
       "3    2022-07-21  31.0  31.6  30.7   30.7          79       106        1   \n",
       "4    2022-07-22  30.7  31.3  30.7   31.3          79       106        1   \n",
       "..          ...   ...   ...   ...    ...         ...       ...      ...   \n",
       "179  2023-03-31  30.5  30.6  30.0   30.0          79       106        1   \n",
       "180  2023-04-03  30.0  30.4  30.0   30.3          79       106        1   \n",
       "181  2023-04-04  30.4  30.8  30.2   30.3          79       106        1   \n",
       "182  2023-04-05  30.8  30.8  30.2   30.6          79       106        1   \n",
       "183  2023-04-06  30.6  30.6  30.2   30.2          79       106        1   \n",
       "\n",
       "     Pattern Present  No Pattern  Rising Wedge  Falling Wedge  Double Top  \\\n",
       "0                  1           0             1              0           0   \n",
       "1                  1           0             1              0           0   \n",
       "2                  1           0             1              0           0   \n",
       "3                  1           0             1              0           0   \n",
       "4                  1           0             1              0           0   \n",
       "..               ...         ...           ...            ...         ...   \n",
       "179                1           0             1              0           0   \n",
       "180                1           0             1              0           0   \n",
       "181                1           0             1              0           0   \n",
       "182                1           0             1              0           0   \n",
       "183                1           0             1              0           0   \n",
       "\n",
       "     Double Bottom  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  \n",
       "..             ...  \n",
       "179              0  \n",
       "180              0  \n",
       "181              0  \n",
       "182              0  \n",
       "183              0  \n",
       "\n",
       "[184 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = one_hot_encode(dfs)\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8388fe-4ade-4dd2-857f-5368223a4bd6",
   "metadata": {},
   "source": [
    "# Select X and Y columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b9b2504-bc91-4e6c-bc42-0f2cfc5555b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_data_with_pattern(pattern: str, model_type) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Take a pattern name and model type and returns a list of arrays.\n",
    "\n",
    "    Args: pattern name: The required pattern name\n",
    "          model_type[Str]: A string to determine the y-output\n",
    "\n",
    "    Returns:\n",
    "        List[Arrays]: A list of arrays with features and target.\n",
    "        \n",
    "    \"\"\"\n",
    "    DATA_PATH_ = Path(f\"data/patterns/{pattern}\")\n",
    "    DATA_PATH_.exists()\n",
    "\n",
    "    data = {filepath.stem: pd.read_csv(filepath) for filepath in DATA_PATH_.glob(\"*.csv\")}\n",
    "\n",
    "    X_real = []\n",
    "    y_real = []\n",
    "    \n",
    "    if model_type == \"full\":\n",
    "        for key, df in data.items():\n",
    "            X_real.append(df[['Open', 'High', 'Low', 'Close']].values)\n",
    "            y_real.append(df.loc[0, ['Start Date', 'End Date', 'Pattern']].values)\n",
    "            \n",
    "    elif model_type == 'categorise':\n",
    "        for key, df in data.items():\n",
    "            X_real.append(df[['Open', 'High', 'Low', 'Close']].values)\n",
    "            y_real.append(df.loc[0, ['Pattern Present']].values)\n",
    "\n",
    "    return X_real, y_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32d3b4fd-efb6-4d7b-850f-542e4203a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(X_real, pattern, model_type, noise=True):\n",
    "    \"\"\"Takes the real X data and pattern and returns the required amount of synthetic data\"\"\"\n",
    "    amount_40 = int(len(X_real) * 0.4)\n",
    "\n",
    "    X_synthetic, y_synthetic = gen_x_y(l=amount_40, pattern=pattern, noise=noise, general=False, model_type=model_type)\n",
    "\n",
    "    return X_synthetic, y_synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00d9af77-9ad2-4d60-98ce-8a10238ab1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_data(*args):\n",
    "    \"\"\"Takes data arrays as arguments and returns a combined list of arrays\"\"\"\n",
    "    combined_data = []\n",
    "    for data in args:\n",
    "        if data is not None:\n",
    "            combined_data.extend(data)\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3582bbc-23bf-4490-98e6-016d1e0171b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_arrays(X, dtype='float32', padding='post', value=-100):\n",
    "    \"\"\"Takes a list of arrays and pads to the minimum required length\"\"\"\n",
    "    return pad_sequences(X, dtype=dtype, padding=padding, value=value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1073f4d-64f6-40cc-8402-bcf4dc665908",
   "metadata": {},
   "source": [
    "# Testing Models to Find Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dde3612-ad18-48ec-9803-027555102386",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea19bdc6-60db-41fd-bd5d-d59f1a606c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_CNN(input_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Masking(mask_value=-100, input_shape=input_shape))\n",
    "    model.add(Conv1D(32, activation='relu', kernel_size=3, kernel_regularizer=regularizers.L1L2(l1=1e-3, l2=1e-3)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(32, activation='relu', kernel_size=3, kernel_regularizer=regularizers.L1L2(l1=1e-3, l2=1e-3)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=200, activation='relu', kernel_regularizer=regularizers.L1L2(l1=1e-3, l2=1e-3)))\n",
    "    model.add(Dense(units=100, activation='relu', kernel_regularizer=regularizers.L1L2(l1=1e-3, l2=1e-3)))\n",
    "    model.add(Dense(units=16, activation='relu', kernel_regularizer=regularizers.L1L2(l1=1e-3, l2=1e-3)))\n",
    "    model.add(Dense(units=16, activation='relu', kernel_regularizer=regularizers.L1L2(l1=1e-3, l2=1e-3)))\n",
    "    model.add(Dense(units=2, activation='linear'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6ed29-9454-45a8-9c8e-0dada5e11dae",
   "metadata": {},
   "source": [
    "## Rising Wedge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5dcbe5f8-d0f6-47d2-86bf-5cfc64421876",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_real, y_real = real_data_with_pattern(pattern='rising_wedge', model_type='full')\n",
    "X_synthetic, y_synthetic = synthetic_data(X_real, 'rising_wedge', model_type='full', noise=False)\n",
    "X_joined = join_data(X_real, X_synthetic)\n",
    "y_joined = join_data(y_real, y_synthetic)\n",
    "X_pad = pad_arrays(X_joined)\n",
    "y_all = np.array(y_joined)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_all, test_size=0.30)\n",
    "\n",
    "# X_train, y_train = data_augmentation(X_train, y_train)\n",
    "\n",
    "y_train = [entry[:2] for entry in y_train]\n",
    "\n",
    "X_train = tf.convert_to_tensor(X_train, np.float32)\n",
    "y_train = tf.convert_to_tensor(y_train, np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "943a6884-1776-4a3e-ba25-90cb36315622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "7/7 [==============================] - 1s 44ms/step - loss: 616082.8125 - mae: 193.7579 - val_loss: 20938.7578 - val_mae: 116.6263\n",
      "Epoch 2/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 25276.0117 - mae: 124.8999 - val_loss: 20126.1172 - val_mae: 113.0089\n",
      "Epoch 3/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 22264.7891 - mae: 119.6539 - val_loss: 18796.0352 - val_mae: 109.4053\n",
      "Epoch 4/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 20710.8223 - mae: 116.3398 - val_loss: 17450.5898 - val_mae: 106.1992\n",
      "Epoch 5/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 19412.1465 - mae: 113.3215 - val_loss: 16295.3379 - val_mae: 102.8776\n",
      "Epoch 6/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 17919.2891 - mae: 109.5918 - val_loss: 14992.6211 - val_mae: 99.2537\n",
      "Epoch 7/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 16429.2832 - mae: 105.3128 - val_loss: 521110.0938 - val_mae: 161.3933\n",
      "Epoch 8/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 197426.2188 - mae: 125.6266 - val_loss: 15648.5654 - val_mae: 99.9186\n",
      "Epoch 9/1000\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 17387.1816 - mae: 107.2536 - val_loss: 16106.2510 - val_mae: 100.4745\n",
      "Epoch 10/1000\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 17016.0078 - mae: 106.2330 - val_loss: 15205.9580 - val_mae: 98.0888\n",
      "Epoch 11/1000\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 15873.8262 - mae: 103.1893 - val_loss: 13566.6572 - val_mae: 91.3675\n",
      "Epoch 12/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 14892.1807 - mae: 98.8478 - val_loss: 13191.7617 - val_mae: 90.7267\n",
      "Epoch 13/1000\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 14492.0605 - mae: 97.0070 - val_loss: 12724.3291 - val_mae: 87.6043\n",
      "Epoch 14/1000\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 14124.0215 - mae: 94.9404 - val_loss: 12616.0283 - val_mae: 87.3018\n",
      "Epoch 15/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 13919.7012 - mae: 93.7262 - val_loss: 12440.9756 - val_mae: 86.0529\n",
      "Epoch 16/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 13892.9844 - mae: 93.7907 - val_loss: 12921.7959 - val_mae: 91.0256\n",
      "Epoch 17/1000\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 14042.0732 - mae: 94.5506 - val_loss: 12702.5176 - val_mae: 89.3545\n",
      "Epoch 18/1000\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 13864.5439 - mae: 94.1821 - val_loss: 12441.6025 - val_mae: 86.8646\n",
      "Epoch 19/1000\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 13639.2988 - mae: 93.4420 - val_loss: 13014.6816 - val_mae: 90.6061\n",
      "Epoch 20/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 13287.6094 - mae: 90.9467 - val_loss: 12460.4912 - val_mae: 86.8556\n",
      "Epoch 21/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 12998.0410 - mae: 88.7967 - val_loss: 11728.3906 - val_mae: 82.1199\n",
      "Epoch 22/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 12391.9756 - mae: 85.4974 - val_loss: 11113.0186 - val_mae: 78.9155\n",
      "Epoch 23/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 12828.8779 - mae: 85.3524 - val_loss: 12583.0664 - val_mae: 88.6293\n",
      "Epoch 24/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 12633.7539 - mae: 87.1680 - val_loss: 10959.9072 - val_mae: 76.9115\n",
      "Epoch 25/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 10950.0176 - mae: 77.8176 - val_loss: 11391.8262 - val_mae: 80.3762\n",
      "Epoch 26/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 9924.2686 - mae: 74.3782 - val_loss: 9230.4863 - val_mae: 69.1463\n",
      "Epoch 27/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 7733.0249 - mae: 63.8693 - val_loss: 5623.4243 - val_mae: 56.8215\n",
      "Epoch 28/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 12673.0566 - mae: 78.6252 - val_loss: 11186.5439 - val_mae: 75.4302\n",
      "Epoch 29/1000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 11313.6094 - mae: 79.2225 - val_loss: 12200.3213 - val_mae: 78.2424\n",
      "Epoch 30/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 10838.3516 - mae: 74.6828 - val_loss: 11134.0684 - val_mae: 74.3355\n",
      "Epoch 31/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 9882.8418 - mae: 71.9456 - val_loss: 11088.0205 - val_mae: 74.2387\n",
      "Epoch 32/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 8960.2393 - mae: 67.5862 - val_loss: 9308.8164 - val_mae: 66.6678\n",
      "Epoch 33/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 8102.6455 - mae: 63.9787 - val_loss: 9815.7617 - val_mae: 74.1204\n",
      "Epoch 34/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 7161.0708 - mae: 60.4780 - val_loss: 6804.2720 - val_mae: 56.6069\n",
      "Epoch 35/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 5919.7930 - mae: 50.7823 - val_loss: 5734.4805 - val_mae: 48.5760\n",
      "Epoch 36/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 5250.4619 - mae: 46.7037 - val_loss: 11020.7871 - val_mae: 49.2328\n",
      "Epoch 37/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 6082.6416 - mae: 52.7913 - val_loss: 6082.1978 - val_mae: 51.7466\n",
      "Epoch 38/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 5687.1646 - mae: 47.0798 - val_loss: 6493.8447 - val_mae: 51.0213\n",
      "Epoch 39/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 5665.9404 - mae: 47.1591 - val_loss: 6379.5732 - val_mae: 51.9811\n",
      "Epoch 40/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 5227.1128 - mae: 44.3640 - val_loss: 5431.3291 - val_mae: 44.7294\n",
      "Epoch 41/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 4866.0103 - mae: 41.4573 - val_loss: 4895.3198 - val_mae: 41.8077\n",
      "Epoch 42/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 4639.8228 - mae: 40.2129 - val_loss: 4662.4844 - val_mae: 41.2950\n",
      "Epoch 43/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 4481.6001 - mae: 39.8466 - val_loss: 4143.3467 - val_mae: 38.6322\n",
      "Epoch 44/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 4222.1777 - mae: 38.0614 - val_loss: 3716.8425 - val_mae: 36.8212\n",
      "Epoch 45/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 3614.3430 - mae: 35.7179 - val_loss: 3244.5588 - val_mae: 36.0253\n",
      "Epoch 46/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 3306.3945 - mae: 34.7187 - val_loss: 3953.4670 - val_mae: 40.8657\n",
      "Epoch 47/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 3766.0095 - mae: 37.1645 - val_loss: 3159.8872 - val_mae: 35.9777\n",
      "Epoch 48/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 3503.1663 - mae: 34.8297 - val_loss: 2744.3484 - val_mae: 34.9870\n",
      "Epoch 49/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 2786.4707 - mae: 33.0909 - val_loss: 3109.8296 - val_mae: 37.7353\n",
      "Epoch 50/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 3483.4653 - mae: 36.9837 - val_loss: 2991.0999 - val_mae: 37.3247\n",
      "Epoch 51/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 3536.0254 - mae: 34.5922 - val_loss: 2713.4253 - val_mae: 34.6913\n",
      "Epoch 52/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 3307.6052 - mae: 34.4595 - val_loss: 2339.8887 - val_mae: 32.4119\n",
      "Epoch 53/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 2754.9043 - mae: 30.9338 - val_loss: 2037.4224 - val_mae: 29.9737\n",
      "Epoch 54/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 2516.5679 - mae: 29.8173 - val_loss: 2211.4888 - val_mae: 32.2890\n",
      "Epoch 55/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 2241.0608 - mae: 29.0272 - val_loss: 2316.0879 - val_mae: 33.3159\n",
      "Epoch 56/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 2747.1558 - mae: 31.2476 - val_loss: 2770.8303 - val_mae: 38.0370\n",
      "Epoch 57/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 2612.7966 - mae: 33.4237 - val_loss: 2286.5686 - val_mae: 32.2068\n",
      "Epoch 58/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 2036.0642 - mae: 28.8900 - val_loss: 2306.5906 - val_mae: 31.8087\n",
      "Epoch 59/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 2127.6262 - mae: 28.4120 - val_loss: 1991.1733 - val_mae: 30.8925\n",
      "Epoch 60/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 1844.1217 - mae: 26.3340 - val_loss: 1889.7477 - val_mae: 29.4583\n",
      "Epoch 61/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1872.2987 - mae: 25.4774 - val_loss: 1978.8530 - val_mae: 31.3169\n",
      "Epoch 62/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1653.3265 - mae: 24.6917 - val_loss: 1834.8010 - val_mae: 29.3404\n",
      "Epoch 63/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1723.6354 - mae: 24.2082 - val_loss: 1710.3069 - val_mae: 29.3138\n",
      "Epoch 64/1000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1355.4515 - mae: 22.8967 - val_loss: 1546.9915 - val_mae: 27.3383\n",
      "Epoch 65/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1258.4563 - mae: 22.1321 - val_loss: 1508.9557 - val_mae: 26.7819\n",
      "Epoch 66/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 1228.2794 - mae: 21.6127 - val_loss: 1438.5345 - val_mae: 27.0332\n",
      "Epoch 67/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1338.5668 - mae: 23.4578 - val_loss: 1672.4652 - val_mae: 26.9812\n",
      "Epoch 68/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1819.3383 - mae: 24.1896 - val_loss: 2007.8037 - val_mae: 30.7226\n",
      "Epoch 69/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 2232.6331 - mae: 27.9630 - val_loss: 1697.4362 - val_mae: 27.4371\n",
      "Epoch 70/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1890.7880 - mae: 25.7271 - val_loss: 2241.4077 - val_mae: 34.1556\n",
      "Epoch 71/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 21018.0684 - mae: 57.6665 - val_loss: 15422.8066 - val_mae: 86.5609\n",
      "Epoch 72/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 9669.9912 - mae: 68.3575 - val_loss: 12395.3311 - val_mae: 78.2924\n",
      "Epoch 73/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 8133.4526 - mae: 59.6910 - val_loss: 9518.2285 - val_mae: 64.3426\n",
      "Epoch 74/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 7098.9209 - mae: 56.5637 - val_loss: 7654.3081 - val_mae: 58.4408\n",
      "Epoch 75/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 6065.0820 - mae: 50.1084 - val_loss: 6425.3027 - val_mae: 50.2591\n",
      "Epoch 76/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 5499.0127 - mae: 46.1205 - val_loss: 6245.6294 - val_mae: 50.0248\n",
      "Epoch 77/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 5130.2188 - mae: 44.8810 - val_loss: 5290.0239 - val_mae: 44.4695\n",
      "Epoch 78/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 4740.3604 - mae: 41.8040 - val_loss: 5117.2168 - val_mae: 42.7813\n",
      "Epoch 79/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 4261.2417 - mae: 39.3941 - val_loss: 4321.3433 - val_mae: 40.5926\n",
      "Epoch 80/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 4036.4639 - mae: 37.2545 - val_loss: 3929.8896 - val_mae: 36.8429\n",
      "Epoch 81/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 3785.1655 - mae: 35.3671 - val_loss: 3157.7004 - val_mae: 35.0652\n",
      "Epoch 82/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 3579.5371 - mae: 35.4583 - val_loss: 2798.7314 - val_mae: 33.5746\n",
      "Epoch 83/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 3298.3914 - mae: 33.2125 - val_loss: 2570.5066 - val_mae: 31.7736\n",
      "Epoch 84/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 3237.6926 - mae: 32.9293 - val_loss: 2362.9834 - val_mae: 31.2228\n",
      "Epoch 85/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 3481.6121 - mae: 31.6887 - val_loss: 2459.6328 - val_mae: 32.7220\n",
      "Epoch 86/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 2740.5059 - mae: 31.3545 - val_loss: 2617.3550 - val_mae: 31.8326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f538f92da50>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(patience = 20, restore_best_weights=True)\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "model = initialize_model_CNN(input_shape)\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split = 0.2,\n",
    "    shuffle = True,\n",
    "    batch_size=64,\n",
    "    epochs = 1000,\n",
    "    callbacks = [es],\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7dcc2a47-61ef-46d0-9757-15ed6f35f1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step - loss: 2743.1082 - mae: 25.8211\n"
     ]
    }
   ],
   "source": [
    "y_test = [entry[:2] for entry in y_test]\n",
    "X_test = tf.convert_to_tensor(X_test, np.float32)\n",
    "y_test = tf.convert_to_tensor(y_test, np.int16)\n",
    "res = model.evaluate(X_test, y_test)\n",
    "\n",
    "results_dict['Rising Wedge'] = res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c3f7e8-1ee0-4259-b64c-579b6e66b425",
   "metadata": {},
   "source": [
    "## Falling Wedge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "dd76b6de-8593-4887-9d80-7931f7c2e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_real, y_real = real_data_with_pattern(pattern='falling_wedge', model_type='full')\n",
    "X_synthetic, y_synthetic = synthetic_data(X_real, 'falling_wedge', model_type='full', noise=False)\n",
    "X_joined = join_data(X_real, X_synthetic)\n",
    "y_joined = join_data(y_real, y_synthetic)\n",
    "\n",
    "# X_joined, y_joined = data_augmentation(X_joined, y_joined)\n",
    "\n",
    "X_pad = pad_arrays(X_joined)\n",
    "y_all = np.array(y_joined)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_all, test_size=0.30)\n",
    "y_train = [entry[:2] for entry in y_train]\n",
    "X_train = tf.convert_to_tensor(X_train, np.float32)\n",
    "y_train = tf.convert_to_tensor(y_train, np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2d8a20b0-bf91-4f5f-950f-e32cbcf14fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4/4 [==============================] - 1s 59ms/step - loss: 100083.2266 - mae: 151.7234 - val_loss: 36820.5352 - val_mae: 141.2957\n",
      "Epoch 2/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 38120.1953 - mae: 127.3838 - val_loss: 36511.7969 - val_mae: 139.1529\n",
      "Epoch 3/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 32722.9980 - mae: 122.5913 - val_loss: 36419.3789 - val_mae: 139.7722\n",
      "Epoch 4/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 29482.8184 - mae: 119.9228 - val_loss: 36020.7617 - val_mae: 136.3114\n",
      "Epoch 5/1000\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 88813.1641 - mae: 144.4091 - val_loss: 35809.9336 - val_mae: 135.5821\n",
      "Epoch 6/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 52472.9531 - mae: 132.8728 - val_loss: 35682.1484 - val_mae: 135.9346\n",
      "Epoch 7/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 31187.2617 - mae: 118.5849 - val_loss: 35515.9453 - val_mae: 135.6390\n",
      "Epoch 8/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 30765.3066 - mae: 118.3405 - val_loss: 35286.3164 - val_mae: 134.1618\n",
      "Epoch 9/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 26064.8809 - mae: 110.5896 - val_loss: 35056.0391 - val_mae: 132.3198\n",
      "Epoch 10/1000\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 25109.2207 - mae: 107.7511 - val_loss: 34801.1680 - val_mae: 132.0228\n",
      "Epoch 11/1000\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 24999.7012 - mae: 108.6720 - val_loss: 34553.1836 - val_mae: 131.4429\n",
      "Epoch 12/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 24722.4004 - mae: 107.1240 - val_loss: 34305.6875 - val_mae: 126.9221\n",
      "Epoch 13/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 24359.2656 - mae: 104.2468 - val_loss: 33250.8945 - val_mae: 126.2638\n",
      "Epoch 14/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 23844.6504 - mae: 104.3330 - val_loss: 32516.4512 - val_mae: 124.7963\n",
      "Epoch 15/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 22844.8613 - mae: 101.4535 - val_loss: 30395.4473 - val_mae: 118.1750\n",
      "Epoch 16/1000\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 21326.8516 - mae: 96.7104 - val_loss: 27754.0430 - val_mae: 113.1680\n",
      "Epoch 17/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 18546.0977 - mae: 91.3114 - val_loss: 20747.7461 - val_mae: 102.8344\n",
      "Epoch 18/1000\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 13198.9355 - mae: 80.8822 - val_loss: 9749.0947 - val_mae: 76.3627\n",
      "Epoch 19/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 13790.8838 - mae: 77.6504 - val_loss: 7650.9385 - val_mae: 71.5997\n",
      "Epoch 20/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 7233.0513 - mae: 66.3477 - val_loss: 6534.1455 - val_mae: 68.3008\n",
      "Epoch 21/1000\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 5679.2065 - mae: 59.7291 - val_loss: 6786.0249 - val_mae: 66.2975\n",
      "Epoch 22/1000\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 5791.1465 - mae: 61.0563 - val_loss: 6052.9751 - val_mae: 63.0206\n",
      "Epoch 23/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 5037.3335 - mae: 56.8025 - val_loss: 5097.2148 - val_mae: 58.6310\n",
      "Epoch 24/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 4655.3662 - mae: 53.2878 - val_loss: 6553.3999 - val_mae: 65.7989\n",
      "Epoch 25/1000\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 4837.1665 - mae: 55.5073 - val_loss: 4935.6797 - val_mae: 57.2285\n",
      "Epoch 26/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 4348.3418 - mae: 51.8023 - val_loss: 6042.9043 - val_mae: 63.3790\n",
      "Epoch 27/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 4371.4346 - mae: 53.0247 - val_loss: 5444.3882 - val_mae: 58.7160\n",
      "Epoch 28/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 6235.6147 - mae: 57.1810 - val_loss: 6352.8799 - val_mae: 65.7270\n",
      "Epoch 29/1000\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 4406.5996 - mae: 54.0263 - val_loss: 4248.9756 - val_mae: 52.9441\n",
      "Epoch 30/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 4144.9287 - mae: 50.5106 - val_loss: 4228.1797 - val_mae: 51.8497\n",
      "Epoch 31/1000\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 3584.9646 - mae: 47.1345 - val_loss: 4946.9526 - val_mae: 56.0573\n",
      "Epoch 32/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 3288.2749 - mae: 43.6063 - val_loss: 3545.6316 - val_mae: 47.2804\n",
      "Epoch 33/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 3079.9978 - mae: 42.0198 - val_loss: 3913.4653 - val_mae: 48.7693\n",
      "Epoch 34/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 2958.1570 - mae: 41.4447 - val_loss: 3199.1604 - val_mae: 43.0172\n",
      "Epoch 35/1000\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 2596.0549 - mae: 37.1173 - val_loss: 3092.8247 - val_mae: 42.2214\n",
      "Epoch 36/1000\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 2387.8950 - mae: 35.0479 - val_loss: 3016.7312 - val_mae: 41.2581\n",
      "Epoch 37/1000\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 2245.5225 - mae: 33.4585 - val_loss: 2851.3972 - val_mae: 39.2379\n",
      "Epoch 38/1000\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 2054.3013 - mae: 31.1020 - val_loss: 2704.5181 - val_mae: 37.8164\n",
      "Epoch 39/1000\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 1951.2704 - mae: 30.3251 - val_loss: 2965.4097 - val_mae: 39.5411\n",
      "Epoch 40/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 1850.3403 - mae: 29.0280 - val_loss: 2605.6479 - val_mae: 35.6331\n",
      "Epoch 41/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 1770.9161 - mae: 27.5465 - val_loss: 2471.1694 - val_mae: 35.7012\n",
      "Epoch 42/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 1629.5890 - mae: 26.8427 - val_loss: 2651.3376 - val_mae: 36.4948\n",
      "Epoch 43/1000\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 1529.6809 - mae: 26.0262 - val_loss: 2497.1326 - val_mae: 35.0171\n",
      "Epoch 44/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 1457.5782 - mae: 25.2750 - val_loss: 2497.4006 - val_mae: 34.7383\n",
      "Epoch 45/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 1411.7249 - mae: 24.8792 - val_loss: 2420.0186 - val_mae: 33.7271\n",
      "Epoch 46/1000\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 1352.4218 - mae: 25.0299 - val_loss: 2426.0869 - val_mae: 33.4948\n",
      "Epoch 47/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 1260.0845 - mae: 24.1296 - val_loss: 2422.1230 - val_mae: 33.8975\n",
      "Epoch 48/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 1326.9984 - mae: 24.6078 - val_loss: 2141.7434 - val_mae: 33.2734\n",
      "Epoch 49/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 1325.4872 - mae: 25.8488 - val_loss: 2539.5159 - val_mae: 36.6324\n",
      "Epoch 50/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 1331.7107 - mae: 26.1390 - val_loss: 2400.2910 - val_mae: 35.3570\n",
      "Epoch 51/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 1228.7985 - mae: 24.5561 - val_loss: 2158.8286 - val_mae: 33.5115\n",
      "Epoch 52/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 1266.9617 - mae: 24.8769 - val_loss: 2153.7212 - val_mae: 33.0060\n",
      "Epoch 53/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 1170.6378 - mae: 24.0537 - val_loss: 2188.6819 - val_mae: 33.8006\n",
      "Epoch 54/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 1147.2012 - mae: 24.0469 - val_loss: 2267.3296 - val_mae: 33.8627\n",
      "Epoch 55/1000\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 1171.3862 - mae: 24.5110 - val_loss: 2347.1125 - val_mae: 33.9309\n",
      "Epoch 56/1000\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 1094.1017 - mae: 23.2947 - val_loss: 2270.1323 - val_mae: 32.5214\n",
      "Epoch 57/1000\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 1075.7731 - mae: 23.0958 - val_loss: 2473.3003 - val_mae: 34.3865\n",
      "Epoch 58/1000\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 1054.8232 - mae: 22.8079 - val_loss: 2388.5474 - val_mae: 33.4075\n",
      "Epoch 59/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 996.1646 - mae: 21.6920 - val_loss: 2213.3938 - val_mae: 31.5917\n",
      "Epoch 60/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 1012.9033 - mae: 21.7098 - val_loss: 2230.5151 - val_mae: 31.6208\n",
      "Epoch 61/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 987.2875 - mae: 22.1769 - val_loss: 2326.1924 - val_mae: 32.5527\n",
      "Epoch 62/1000\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 1036.1281 - mae: 22.3830 - val_loss: 2138.3115 - val_mae: 30.7972\n",
      "Epoch 63/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 1003.4514 - mae: 22.5886 - val_loss: 2179.0874 - val_mae: 31.4199\n",
      "Epoch 64/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 910.7428 - mae: 20.8605 - val_loss: 2546.9929 - val_mae: 34.3489\n",
      "Epoch 65/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 912.1500 - mae: 20.7256 - val_loss: 2249.6533 - val_mae: 31.4293\n",
      "Epoch 66/1000\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 877.7518 - mae: 19.9721 - val_loss: 2333.4966 - val_mae: 31.9921\n",
      "Epoch 67/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 839.0405 - mae: 19.3826 - val_loss: 2298.5195 - val_mae: 31.8025\n",
      "Epoch 68/1000\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 841.0911 - mae: 20.5909 - val_loss: 2304.7148 - val_mae: 32.0205\n",
      "Epoch 69/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 847.5280 - mae: 20.1748 - val_loss: 2213.7312 - val_mae: 30.5263\n",
      "Epoch 70/1000\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 787.1721 - mae: 19.4194 - val_loss: 2221.1807 - val_mae: 30.1552\n",
      "Epoch 71/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 778.0945 - mae: 18.5161 - val_loss: 2174.6755 - val_mae: 29.6566\n",
      "Epoch 72/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 737.7733 - mae: 17.9431 - val_loss: 2147.4949 - val_mae: 29.8899\n",
      "Epoch 73/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 755.5136 - mae: 17.8362 - val_loss: 2096.5776 - val_mae: 29.1194\n",
      "Epoch 74/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 819.5377 - mae: 20.3693 - val_loss: 2493.9910 - val_mae: 32.5592\n",
      "Epoch 75/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 900.0202 - mae: 20.1822 - val_loss: 2175.2883 - val_mae: 30.0228\n",
      "Epoch 76/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 735.6042 - mae: 18.2501 - val_loss: 1856.6132 - val_mae: 28.3737\n",
      "Epoch 77/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 727.9930 - mae: 18.0280 - val_loss: 2115.5217 - val_mae: 29.7865\n",
      "Epoch 78/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 646.4106 - mae: 16.8157 - val_loss: 2268.6692 - val_mae: 29.8426\n",
      "Epoch 79/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 658.2325 - mae: 17.1206 - val_loss: 2182.3760 - val_mae: 28.6720\n",
      "Epoch 80/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 649.2215 - mae: 16.8050 - val_loss: 2072.5645 - val_mae: 28.2131\n",
      "Epoch 81/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 695.5959 - mae: 17.5670 - val_loss: 2456.3306 - val_mae: 32.2556\n",
      "Epoch 82/1000\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 730.1039 - mae: 19.1330 - val_loss: 2318.8835 - val_mae: 31.7881\n",
      "Epoch 83/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 639.4434 - mae: 16.7521 - val_loss: 2012.0483 - val_mae: 28.8316\n",
      "Epoch 84/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 612.2706 - mae: 15.9994 - val_loss: 2080.5601 - val_mae: 28.8827\n",
      "Epoch 85/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 577.7562 - mae: 15.4125 - val_loss: 2209.6194 - val_mae: 29.9859\n",
      "Epoch 86/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 553.6884 - mae: 15.9366 - val_loss: 2019.2345 - val_mae: 28.0238\n",
      "Epoch 87/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 537.2758 - mae: 15.0138 - val_loss: 2050.4348 - val_mae: 28.0728\n",
      "Epoch 88/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 612.0325 - mae: 16.5882 - val_loss: 2332.2180 - val_mae: 30.8926\n",
      "Epoch 89/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 637.9893 - mae: 16.6362 - val_loss: 1969.7074 - val_mae: 28.2059\n",
      "Epoch 90/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 578.3738 - mae: 16.5257 - val_loss: 1950.8223 - val_mae: 28.1946\n",
      "Epoch 91/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 545.3512 - mae: 15.8124 - val_loss: 1999.2626 - val_mae: 28.6010\n",
      "Epoch 92/1000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 511.2151 - mae: 15.8181 - val_loss: 1973.7231 - val_mae: 28.0113\n",
      "Epoch 93/1000\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 459.0149 - mae: 13.6419 - val_loss: 1875.6656 - val_mae: 27.0857\n",
      "Epoch 94/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 447.0366 - mae: 13.8366 - val_loss: 1872.1956 - val_mae: 27.2135\n",
      "Epoch 95/1000\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 432.9217 - mae: 13.6556 - val_loss: 1919.6086 - val_mae: 27.6871\n",
      "Epoch 96/1000\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 424.4297 - mae: 13.4799 - val_loss: 1874.9275 - val_mae: 27.2860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f538faaee30>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(patience = 20, restore_best_weights=True)\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "model = initialize_model_CNN(input_shape)\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split = 0.2,\n",
    "    shuffle = True,\n",
    "    batch_size=64,\n",
    "    epochs = 1000,\n",
    "    callbacks = [es],\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6a340287-ebb0-4d49-9725-37679635b08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 6ms/step - loss: 37037.8516 - mae: 41.8227\n"
     ]
    }
   ],
   "source": [
    "y_test = [entry[:2] for entry in y_test]\n",
    "X_test = tf.convert_to_tensor(X_test, np.float32)\n",
    "y_test = tf.convert_to_tensor(y_test, np.int16)\n",
    "res = model.evaluate(X_test, y_test)\n",
    "\n",
    "results_dict['Falling Wedge'] = res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc17e99a-1620-4a60-9ce1-6acdf1f6499f",
   "metadata": {},
   "source": [
    "## Double Top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3b9c8f2c-2cd0-483c-a5b0-883617779b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_real, y_real = real_data_with_pattern(pattern='double_top', model_type='full')\n",
    "X_synthetic, y_synthetic = synthetic_data(X_real, 'double_top', model_type='full', noise=False)\n",
    "X_joined = join_data(X_real, X_synthetic)\n",
    "y_joined = join_data(y_real, y_synthetic)\n",
    "\n",
    "# X_joined, y_joined = data_augmentation(X_joined, y_joined)\n",
    "\n",
    "X_pad = pad_arrays(X_joined)\n",
    "y_all = np.array(y_joined)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_all, test_size=0.30)\n",
    "y_train = [entry[:2] for entry in y_train]\n",
    "X_train = tf.convert_to_tensor(X_train, np.float32)\n",
    "y_train = tf.convert_to_tensor(y_train, np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7e93a9ed-416d-4799-bb6e-0f9456ce6cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "5/5 [==============================] - 1s 51ms/step - loss: 310481.4062 - mae: 145.2218 - val_loss: 10443.9844 - val_mae: 65.0997\n",
      "Epoch 2/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1010718.2500 - mae: 139.6114 - val_loss: 8730.6084 - val_mae: 63.7014\n",
      "Epoch 3/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 72536.0938 - mae: 87.8817 - val_loss: 8938.8760 - val_mae: 64.8993\n",
      "Epoch 4/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1284827.1250 - mae: 124.7354 - val_loss: 8894.1201 - val_mae: 63.8179\n",
      "Epoch 5/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 69158.0547 - mae: 85.5967 - val_loss: 10116.7412 - val_mae: 66.9876\n",
      "Epoch 6/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 74894.3984 - mae: 83.4267 - val_loss: 9479.8291 - val_mae: 65.1560\n",
      "Epoch 7/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 25300.8184 - mae: 68.4081 - val_loss: 8471.2822 - val_mae: 62.7737\n",
      "Epoch 8/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11756.1855 - mae: 61.1352 - val_loss: 7854.3281 - val_mae: 61.0601\n",
      "Epoch 9/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7017.2295 - mae: 57.0404 - val_loss: 7565.8467 - val_mae: 60.0182\n",
      "Epoch 10/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 33273.4297 - mae: 60.6884 - val_loss: 7611.6357 - val_mae: 59.8431\n",
      "Epoch 11/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 117020.0000 - mae: 73.6634 - val_loss: 7502.9385 - val_mae: 59.4330\n",
      "Epoch 12/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7293.8491 - mae: 54.1913 - val_loss: 7429.7095 - val_mae: 59.0394\n",
      "Epoch 13/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7017.3833 - mae: 53.0577 - val_loss: 7526.0186 - val_mae: 58.6746\n",
      "Epoch 14/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 20314.9668 - mae: 54.8961 - val_loss: 7334.1924 - val_mae: 57.7722\n",
      "Epoch 15/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 269723.8438 - mae: 84.1064 - val_loss: 6749.9595 - val_mae: 53.8239\n",
      "Epoch 16/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2751325.0000 - mae: 129.0406 - val_loss: 7217.9868 - val_mae: 57.7541\n",
      "Epoch 17/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 42406.6406 - mae: 68.7929 - val_loss: 7144.4722 - val_mae: 58.0257\n",
      "Epoch 18/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 41953.3320 - mae: 65.9843 - val_loss: 7099.5830 - val_mae: 58.3065\n",
      "Epoch 19/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 38134.7578 - mae: 61.8442 - val_loss: 7198.2207 - val_mae: 58.2569\n",
      "Epoch 20/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 26370.1484 - mae: 56.9321 - val_loss: 6823.3052 - val_mae: 57.3138\n",
      "Epoch 21/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 19321.9590 - mae: 54.0761 - val_loss: 6321.5054 - val_mae: 55.6744\n",
      "Epoch 22/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 13957.2842 - mae: 50.3957 - val_loss: 5774.0859 - val_mae: 53.6313\n",
      "Epoch 23/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11624.0127 - mae: 47.3097 - val_loss: 5478.5420 - val_mae: 52.0860\n",
      "Epoch 24/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10533.7910 - mae: 44.8436 - val_loss: 5125.2485 - val_mae: 50.7066\n",
      "Epoch 25/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8898.4795 - mae: 42.9633 - val_loss: 5152.9062 - val_mae: 50.3884\n",
      "Epoch 26/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7468.9595 - mae: 41.5186 - val_loss: 4942.9072 - val_mae: 49.7033\n",
      "Epoch 27/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6824.3408 - mae: 41.2543 - val_loss: 5105.8257 - val_mae: 49.2104\n",
      "Epoch 28/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6045.8076 - mae: 39.9169 - val_loss: 4465.4634 - val_mae: 47.6427\n",
      "Epoch 29/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5474.3760 - mae: 38.4608 - val_loss: 4984.8374 - val_mae: 47.3637\n",
      "Epoch 30/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5053.9966 - mae: 38.2464 - val_loss: 3982.9233 - val_mae: 45.0353\n",
      "Epoch 31/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4689.0083 - mae: 36.8059 - val_loss: 4901.8438 - val_mae: 45.4320\n",
      "Epoch 32/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4268.3091 - mae: 35.1762 - val_loss: 3818.8896 - val_mae: 43.0636\n",
      "Epoch 33/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3680.7415 - mae: 33.6111 - val_loss: 3869.3540 - val_mae: 41.0400\n",
      "Epoch 34/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3537.4504 - mae: 32.6131 - val_loss: 3074.9158 - val_mae: 38.6096\n",
      "Epoch 35/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 3241.2698 - mae: 31.6408 - val_loss: 3479.7480 - val_mae: 37.9209\n",
      "Epoch 36/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 2861.4446 - mae: 30.0971 - val_loss: 3244.6531 - val_mae: 37.3027\n",
      "Epoch 37/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 2721.0691 - mae: 27.6986 - val_loss: 3189.5042 - val_mae: 35.9172\n",
      "Epoch 38/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2437.6819 - mae: 26.2037 - val_loss: 3022.2507 - val_mae: 34.8027\n",
      "Epoch 39/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2169.9080 - mae: 24.9913 - val_loss: 2922.2778 - val_mae: 33.8007\n",
      "Epoch 40/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1982.5931 - mae: 23.9330 - val_loss: 2815.1523 - val_mae: 32.7430\n",
      "Epoch 41/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 1877.0490 - mae: 22.9710 - val_loss: 2641.9773 - val_mae: 31.6924\n",
      "Epoch 42/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 1713.2723 - mae: 22.2166 - val_loss: 2621.9106 - val_mae: 31.3795\n",
      "Epoch 43/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1598.6354 - mae: 21.9238 - val_loss: 2472.2407 - val_mae: 30.8023\n",
      "Epoch 44/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1484.6877 - mae: 21.2125 - val_loss: 2428.7310 - val_mae: 30.4430\n",
      "Epoch 45/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1416.7422 - mae: 20.6782 - val_loss: 2353.3257 - val_mae: 29.6462\n",
      "Epoch 46/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1323.9279 - mae: 20.1417 - val_loss: 2284.8228 - val_mae: 29.0091\n",
      "Epoch 47/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1227.7609 - mae: 19.3249 - val_loss: 2136.3252 - val_mae: 28.0090\n",
      "Epoch 48/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1183.5154 - mae: 18.5790 - val_loss: 2145.6030 - val_mae: 27.5804\n",
      "Epoch 49/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1093.3098 - mae: 18.0689 - val_loss: 2060.5283 - val_mae: 26.9622\n",
      "Epoch 50/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1027.2659 - mae: 17.6819 - val_loss: 2048.2249 - val_mae: 26.4276\n",
      "Epoch 51/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 940.8832 - mae: 17.1069 - val_loss: 2014.4807 - val_mae: 25.9039\n",
      "Epoch 52/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 918.1662 - mae: 16.4328 - val_loss: 1952.7329 - val_mae: 25.3904\n",
      "Epoch 53/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 844.8923 - mae: 15.9423 - val_loss: 1984.0333 - val_mae: 25.2186\n",
      "Epoch 54/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 803.1559 - mae: 15.3281 - val_loss: 1842.1920 - val_mae: 24.3387\n",
      "Epoch 55/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 745.3712 - mae: 14.8910 - val_loss: 1942.7413 - val_mae: 24.1985\n",
      "Epoch 56/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 707.5643 - mae: 14.1175 - val_loss: 1791.9323 - val_mae: 23.3147\n",
      "Epoch 57/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 642.2436 - mae: 13.3142 - val_loss: 1821.6746 - val_mae: 23.4673\n",
      "Epoch 58/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 590.1888 - mae: 12.8245 - val_loss: 1816.9546 - val_mae: 23.3820\n",
      "Epoch 59/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 655.5573 - mae: 12.5567 - val_loss: 1727.0009 - val_mae: 22.6386\n",
      "Epoch 60/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 558.4788 - mae: 12.2433 - val_loss: 1701.6589 - val_mae: 22.3963\n",
      "Epoch 61/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 506.3830 - mae: 11.3401 - val_loss: 1704.5625 - val_mae: 21.9062\n",
      "Epoch 62/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 454.7067 - mae: 10.4981 - val_loss: 1620.5039 - val_mae: 21.5080\n",
      "Epoch 63/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 403.8916 - mae: 10.1164 - val_loss: 1694.2299 - val_mae: 21.4550\n",
      "Epoch 64/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 382.0173 - mae: 9.6440 - val_loss: 1619.8307 - val_mae: 21.0848\n",
      "Epoch 65/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 339.3471 - mae: 9.0936 - val_loss: 1598.6199 - val_mae: 20.4834\n",
      "Epoch 66/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 309.0782 - mae: 8.5529 - val_loss: 1702.7424 - val_mae: 20.5277\n",
      "Epoch 67/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 289.0056 - mae: 8.5607 - val_loss: 1535.2543 - val_mae: 19.5281\n",
      "Epoch 68/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 273.0673 - mae: 8.1785 - val_loss: 1662.9960 - val_mae: 19.9476\n",
      "Epoch 69/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 255.6905 - mae: 7.8379 - val_loss: 1668.9177 - val_mae: 19.6903\n",
      "Epoch 70/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 249.7153 - mae: 8.0351 - val_loss: 1536.8148 - val_mae: 19.5235\n",
      "Epoch 71/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 237.7949 - mae: 7.8034 - val_loss: 1556.5105 - val_mae: 19.3015\n",
      "Epoch 72/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 222.7826 - mae: 7.3303 - val_loss: 1646.9233 - val_mae: 19.6608\n",
      "Epoch 73/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 216.6084 - mae: 7.3617 - val_loss: 1566.1455 - val_mae: 19.5460\n",
      "Epoch 74/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 225.3444 - mae: 7.6007 - val_loss: 1621.1281 - val_mae: 19.8078\n",
      "Epoch 75/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 212.0165 - mae: 7.4175 - val_loss: 1659.1565 - val_mae: 19.3687\n",
      "Epoch 76/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 199.7874 - mae: 7.0783 - val_loss: 1622.7539 - val_mae: 19.2443\n",
      "Epoch 77/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 188.4445 - mae: 6.7239 - val_loss: 1667.1573 - val_mae: 19.0191\n",
      "Epoch 78/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 175.7464 - mae: 6.3654 - val_loss: 1620.5405 - val_mae: 19.1386\n",
      "Epoch 79/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 168.4036 - mae: 6.2469 - val_loss: 1670.8868 - val_mae: 19.1629\n",
      "Epoch 80/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 160.9705 - mae: 6.0750 - val_loss: 1601.6499 - val_mae: 18.6490\n",
      "Epoch 81/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 154.6360 - mae: 5.7304 - val_loss: 1514.2966 - val_mae: 18.4252\n",
      "Epoch 82/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 149.4848 - mae: 5.7035 - val_loss: 1655.5233 - val_mae: 18.9665\n",
      "Epoch 83/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 143.7257 - mae: 5.4741 - val_loss: 1586.2919 - val_mae: 18.4778\n",
      "Epoch 84/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 138.9434 - mae: 5.4395 - val_loss: 1587.8748 - val_mae: 18.3772\n",
      "Epoch 85/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 134.8402 - mae: 5.1522 - val_loss: 1686.7650 - val_mae: 18.6636\n",
      "Epoch 86/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 130.7076 - mae: 5.0168 - val_loss: 1584.0479 - val_mae: 18.1824\n",
      "Epoch 87/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 125.9651 - mae: 4.9171 - val_loss: 1588.2856 - val_mae: 18.1640\n",
      "Epoch 88/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 123.2403 - mae: 4.8169 - val_loss: 1673.5952 - val_mae: 18.5413\n",
      "Epoch 89/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 119.8168 - mae: 4.6486 - val_loss: 1650.9473 - val_mae: 18.3975\n",
      "Epoch 90/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 116.9981 - mae: 4.6313 - val_loss: 1583.0842 - val_mae: 18.1669\n",
      "Epoch 91/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 114.9762 - mae: 4.6490 - val_loss: 1625.6294 - val_mae: 18.2397\n",
      "Epoch 92/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 112.2597 - mae: 4.4582 - val_loss: 1675.5378 - val_mae: 18.6246\n",
      "Epoch 93/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 109.7853 - mae: 4.5082 - val_loss: 1576.4132 - val_mae: 18.0815\n",
      "Epoch 94/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 109.0875 - mae: 4.4886 - val_loss: 1650.8203 - val_mae: 18.2630\n",
      "Epoch 95/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 108.2745 - mae: 4.5208 - val_loss: 1755.8135 - val_mae: 18.9053\n",
      "Epoch 96/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 105.6422 - mae: 4.4790 - val_loss: 1633.3829 - val_mae: 18.1550\n",
      "Epoch 97/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 100.7821 - mae: 4.0596 - val_loss: 1595.3086 - val_mae: 18.0229\n",
      "Epoch 98/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 98.8990 - mae: 3.9772 - val_loss: 1648.1785 - val_mae: 18.2076\n",
      "Epoch 99/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 98.3154 - mae: 4.0364 - val_loss: 1645.3146 - val_mae: 18.1913\n",
      "Epoch 100/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 96.3930 - mae: 4.0421 - val_loss: 1617.8961 - val_mae: 18.1027\n",
      "Epoch 101/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 95.8953 - mae: 4.0739 - val_loss: 1685.1548 - val_mae: 18.2954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f538fc7fb50>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(patience = 20, restore_best_weights=True)\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "model = initialize_model_CNN(input_shape)\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split = 0.2,\n",
    "    shuffle = True,\n",
    "    batch_size=64,\n",
    "    epochs = 1000,\n",
    "    callbacks = [es],\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "271e7659-dae3-4b65-aa5d-b76f64102ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 2133.9692 - mae: 23.2194\n"
     ]
    }
   ],
   "source": [
    "y_test = [entry[:2] for entry in y_test]\n",
    "X_test = tf.convert_to_tensor(X_test, np.float32)\n",
    "y_test = tf.convert_to_tensor(y_test, np.int16)\n",
    "res = model.evaluate(X_test, y_test)\n",
    "\n",
    "results_dict['Double Top'] = res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2657d635-be4e-4a90-8e7f-2c8db0efc595",
   "metadata": {},
   "source": [
    "## Double Bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1d8f4868-76ee-4fc0-9948-90817567f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_real, y_real = real_data_with_pattern(pattern='double_bottom', model_type='full')\n",
    "X_synthetic, y_synthetic = synthetic_data(X_real, 'double_bottom', model_type='full', noise=False)\n",
    "X_joined = join_data(X_real, X_synthetic)\n",
    "y_joined = join_data(y_real, y_synthetic)\n",
    "\n",
    "# X_joined, y_joined = data_augmentation(X_joined, y_joined)\n",
    "\n",
    "X_pad = pad_arrays(X_joined)\n",
    "y_all = np.array(y_joined)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_all, test_size=0.30)\n",
    "y_train = [entry[:2] for entry in y_train]\n",
    "y_test = [entry[:2] for entry in y_test]\n",
    "X_train = tf.convert_to_tensor(X_train, np.float32)\n",
    "y_train = tf.convert_to_tensor(y_train, np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a8f6eccc-c3bc-4ac7-b1b1-03c85bb06678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "5/5 [==============================] - 1s 48ms/step - loss: 64931.4492 - mae: 114.4439 - val_loss: 10923.5176 - val_mae: 73.9208\n",
      "Epoch 2/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 13054.7217 - mae: 71.8868 - val_loss: 11229.1602 - val_mae: 72.0320\n",
      "Epoch 3/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 20171.8223 - mae: 69.4417 - val_loss: 9244.2275 - val_mae: 63.8184\n",
      "Epoch 4/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6808.4341 - mae: 54.8172 - val_loss: 6253.9258 - val_mae: 55.0523\n",
      "Epoch 5/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6587.0225 - mae: 51.9531 - val_loss: 6028.4253 - val_mae: 55.0172\n",
      "Epoch 6/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5397.0054 - mae: 47.3328 - val_loss: 5264.3604 - val_mae: 51.7703\n",
      "Epoch 7/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3620.9487 - mae: 44.6156 - val_loss: 4984.4922 - val_mae: 50.5835\n",
      "Epoch 8/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3893.2864 - mae: 44.1328 - val_loss: 4721.7441 - val_mae: 48.6908\n",
      "Epoch 9/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3396.4182 - mae: 42.7863 - val_loss: 4642.4463 - val_mae: 48.0756\n",
      "Epoch 10/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 2737.1699 - mae: 38.9427 - val_loss: 4723.2695 - val_mae: 48.0364\n",
      "Epoch 11/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 2425.2974 - mae: 36.7068 - val_loss: 4253.5708 - val_mae: 45.8258\n",
      "Epoch 12/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 2282.2363 - mae: 35.5041 - val_loss: 4232.7095 - val_mae: 44.7878\n",
      "Epoch 13/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 2193.2676 - mae: 34.2377 - val_loss: 4213.0688 - val_mae: 43.6316\n",
      "Epoch 14/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1919.4519 - mae: 32.2588 - val_loss: 3997.9719 - val_mae: 41.8194\n",
      "Epoch 15/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1881.7775 - mae: 31.7621 - val_loss: 3628.0901 - val_mae: 39.7010\n",
      "Epoch 16/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1749.8936 - mae: 30.0743 - val_loss: 3588.6479 - val_mae: 38.7856\n",
      "Epoch 17/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1645.7307 - mae: 28.0234 - val_loss: 3495.9524 - val_mae: 37.2282\n",
      "Epoch 18/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1434.3260 - mae: 25.7795 - val_loss: 3477.4126 - val_mae: 35.7089\n",
      "Epoch 19/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1256.2784 - mae: 23.4097 - val_loss: 3315.9551 - val_mae: 34.2302\n",
      "Epoch 20/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1156.5983 - mae: 21.7531 - val_loss: 3307.4951 - val_mae: 33.3282\n",
      "Epoch 21/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1420.2219 - mae: 24.8726 - val_loss: 4031.8896 - val_mae: 36.3018\n",
      "Epoch 22/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1250.8132 - mae: 22.5064 - val_loss: 2761.8381 - val_mae: 31.3249\n",
      "Epoch 23/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5492.1011 - mae: 26.8952 - val_loss: 2705.6777 - val_mae: 32.0431\n",
      "Epoch 24/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 2686.3145 - mae: 24.9795 - val_loss: 3799.6636 - val_mae: 37.0677\n",
      "Epoch 25/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1385.8781 - mae: 23.5730 - val_loss: 3082.3699 - val_mae: 34.3748\n",
      "Epoch 26/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1855.0800 - mae: 23.0152 - val_loss: 3084.2583 - val_mae: 34.6253\n",
      "Epoch 27/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1020.2768 - mae: 20.9505 - val_loss: 2586.3323 - val_mae: 32.4176\n",
      "Epoch 28/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1074.9723 - mae: 20.5649 - val_loss: 2710.8494 - val_mae: 33.6328\n",
      "Epoch 29/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 743.5504 - mae: 19.1752 - val_loss: 2705.1077 - val_mae: 33.2380\n",
      "Epoch 30/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 698.1232 - mae: 18.2122 - val_loss: 2899.0840 - val_mae: 32.9962\n",
      "Epoch 31/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 658.9804 - mae: 17.2204 - val_loss: 2644.4280 - val_mae: 31.4536\n",
      "Epoch 32/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 566.0975 - mae: 15.9714 - val_loss: 2473.6521 - val_mae: 30.8133\n",
      "Epoch 33/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 636.8724 - mae: 16.4363 - val_loss: 2413.8962 - val_mae: 30.0491\n",
      "Epoch 34/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 588.9278 - mae: 15.3363 - val_loss: 2665.0522 - val_mae: 30.0769\n",
      "Epoch 35/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 550.0590 - mae: 15.2790 - val_loss: 2335.0127 - val_mae: 28.4955\n",
      "Epoch 36/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 512.5037 - mae: 14.9813 - val_loss: 2504.4480 - val_mae: 29.3995\n",
      "Epoch 37/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 452.0977 - mae: 13.9416 - val_loss: 2118.6257 - val_mae: 26.8265\n",
      "Epoch 38/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 500.0409 - mae: 13.4065 - val_loss: 2355.1426 - val_mae: 27.3991\n",
      "Epoch 39/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 438.1893 - mae: 13.1654 - val_loss: 2252.5593 - val_mae: 26.7912\n",
      "Epoch 40/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 394.2356 - mae: 12.6033 - val_loss: 2316.2349 - val_mae: 26.1337\n",
      "Epoch 41/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 346.9613 - mae: 12.0860 - val_loss: 2041.4888 - val_mae: 25.6854\n",
      "Epoch 42/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 334.7708 - mae: 12.3453 - val_loss: 2058.4482 - val_mae: 24.9044\n",
      "Epoch 43/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 319.9639 - mae: 11.2666 - val_loss: 2197.0229 - val_mae: 24.9288\n",
      "Epoch 44/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 292.0022 - mae: 10.7299 - val_loss: 2170.3586 - val_mae: 24.9608\n",
      "Epoch 45/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 275.9646 - mae: 11.0877 - val_loss: 2138.3716 - val_mae: 24.7873\n",
      "Epoch 46/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 260.2087 - mae: 10.5694 - val_loss: 2023.1416 - val_mae: 24.1186\n",
      "Epoch 47/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 270.1778 - mae: 10.8911 - val_loss: 1816.9736 - val_mae: 22.9521\n",
      "Epoch 48/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 253.3066 - mae: 10.0500 - val_loss: 1978.1005 - val_mae: 23.1040\n",
      "Epoch 49/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 297.8217 - mae: 9.8776 - val_loss: 2117.8694 - val_mae: 23.3411\n",
      "Epoch 50/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 219.7000 - mae: 9.6680 - val_loss: 1813.1913 - val_mae: 22.4853\n",
      "Epoch 51/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 239.5426 - mae: 10.0242 - val_loss: 2358.2280 - val_mae: 24.2042\n",
      "Epoch 52/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 240.7442 - mae: 10.3724 - val_loss: 1764.4380 - val_mae: 22.3866\n",
      "Epoch 53/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 224.3531 - mae: 10.2969 - val_loss: 1639.7627 - val_mae: 21.2881\n",
      "Epoch 54/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 195.3113 - mae: 8.9467 - val_loss: 2192.2551 - val_mae: 23.3258\n",
      "Epoch 55/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 204.9276 - mae: 9.2599 - val_loss: 1466.9385 - val_mae: 19.9083\n",
      "Epoch 56/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 189.5511 - mae: 9.2439 - val_loss: 1578.1790 - val_mae: 20.5955\n",
      "Epoch 57/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 178.9310 - mae: 8.7955 - val_loss: 2058.3723 - val_mae: 22.3798\n",
      "Epoch 58/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 172.4579 - mae: 8.3171 - val_loss: 1269.0107 - val_mae: 18.8969\n",
      "Epoch 59/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 177.2711 - mae: 8.5039 - val_loss: 1648.7152 - val_mae: 20.7723\n",
      "Epoch 60/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 155.1369 - mae: 8.2314 - val_loss: 1639.9559 - val_mae: 20.8296\n",
      "Epoch 61/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 144.2599 - mae: 7.6515 - val_loss: 1396.1333 - val_mae: 19.5146\n",
      "Epoch 62/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 175.3027 - mae: 7.8790 - val_loss: 1777.4950 - val_mae: 21.4318\n",
      "Epoch 63/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 264.7878 - mae: 9.2532 - val_loss: 1428.6255 - val_mae: 19.6607\n",
      "Epoch 64/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 304.6668 - mae: 8.4826 - val_loss: 1506.6172 - val_mae: 19.7286\n",
      "Epoch 65/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 169.7398 - mae: 7.9545 - val_loss: 1621.3405 - val_mae: 20.0729\n",
      "Epoch 66/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 209.8584 - mae: 8.4905 - val_loss: 1455.3323 - val_mae: 19.1469\n",
      "Epoch 67/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 147.0737 - mae: 7.5557 - val_loss: 1473.9449 - val_mae: 19.6776\n",
      "Epoch 68/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 189.5338 - mae: 7.8442 - val_loss: 1499.0938 - val_mae: 19.2878\n",
      "Epoch 69/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 137.7785 - mae: 6.8481 - val_loss: 1637.6760 - val_mae: 19.7181\n",
      "Epoch 70/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 135.7316 - mae: 7.0595 - val_loss: 1282.0875 - val_mae: 18.5931\n",
      "Epoch 71/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 130.4484 - mae: 6.7883 - val_loss: 1562.6382 - val_mae: 19.1742\n",
      "Epoch 72/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 127.2862 - mae: 6.7236 - val_loss: 1720.0950 - val_mae: 19.8651\n",
      "Epoch 73/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 124.5710 - mae: 6.8329 - val_loss: 1159.2002 - val_mae: 17.6196\n",
      "Epoch 74/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 126.7432 - mae: 6.5692 - val_loss: 1502.4115 - val_mae: 19.1441\n",
      "Epoch 75/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 132.9792 - mae: 7.0784 - val_loss: 1638.6805 - val_mae: 20.0140\n",
      "Epoch 76/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 118.5297 - mae: 6.6750 - val_loss: 1228.7190 - val_mae: 18.3637\n",
      "Epoch 77/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 137.5128 - mae: 7.0966 - val_loss: 1971.8727 - val_mae: 20.6649\n",
      "Epoch 78/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 205.9292 - mae: 8.7336 - val_loss: 1335.7418 - val_mae: 18.6173\n",
      "Epoch 79/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 188.7582 - mae: 8.7827 - val_loss: 1226.0361 - val_mae: 17.8079\n",
      "Epoch 80/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 138.8452 - mae: 7.2279 - val_loss: 1714.9923 - val_mae: 19.4980\n",
      "Epoch 81/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 118.2947 - mae: 6.6032 - val_loss: 1048.5663 - val_mae: 17.1751\n",
      "Epoch 82/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 147.2509 - mae: 7.3644 - val_loss: 1335.6654 - val_mae: 17.9939\n",
      "Epoch 83/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 134.6668 - mae: 6.9279 - val_loss: 1429.8070 - val_mae: 18.0646\n",
      "Epoch 84/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 120.3477 - mae: 6.6907 - val_loss: 1199.8661 - val_mae: 17.4316\n",
      "Epoch 85/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 120.7904 - mae: 6.5089 - val_loss: 1513.0558 - val_mae: 19.1407\n",
      "Epoch 86/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 113.7966 - mae: 6.5604 - val_loss: 1134.1957 - val_mae: 17.2603\n",
      "Epoch 87/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 102.9662 - mae: 6.0354 - val_loss: 1432.4139 - val_mae: 18.4529\n",
      "Epoch 88/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 101.7464 - mae: 5.9027 - val_loss: 1370.1300 - val_mae: 18.0481\n",
      "Epoch 89/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 95.1917 - mae: 5.7827 - val_loss: 1281.8466 - val_mae: 17.3461\n",
      "Epoch 90/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 91.7493 - mae: 5.3540 - val_loss: 1300.6615 - val_mae: 17.3156\n",
      "Epoch 91/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 90.5898 - mae: 5.2509 - val_loss: 1256.5907 - val_mae: 17.1910\n",
      "Epoch 92/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 87.0716 - mae: 5.0681 - val_loss: 1297.4497 - val_mae: 17.2739\n",
      "Epoch 93/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 92.5874 - mae: 4.9609 - val_loss: 1247.1166 - val_mae: 17.1704\n",
      "Epoch 94/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 82.3874 - mae: 4.7575 - val_loss: 1329.0125 - val_mae: 17.6551\n",
      "Epoch 95/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 87.2702 - mae: 4.8478 - val_loss: 1336.7939 - val_mae: 17.4118\n",
      "Epoch 96/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 83.0680 - mae: 4.5606 - val_loss: 1353.2552 - val_mae: 17.4299\n",
      "Epoch 97/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 100.7594 - mae: 5.0791 - val_loss: 1319.8558 - val_mae: 17.5201\n",
      "Epoch 98/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 161.8299 - mae: 5.2475 - val_loss: 1338.6742 - val_mae: 17.3460\n",
      "Epoch 99/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 210.3111 - mae: 5.5825 - val_loss: 1329.3834 - val_mae: 17.2012\n",
      "Epoch 100/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3191.9111 - mae: 9.0422 - val_loss: 1283.4169 - val_mae: 18.1790\n",
      "Epoch 101/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 306.6059 - mae: 8.7293 - val_loss: 1693.3364 - val_mae: 18.3729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f538f0f3640>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(patience = 20, restore_best_weights=True)\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "model = initialize_model_CNN(input_shape)\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split = 0.2,\n",
    "    shuffle = True,\n",
    "    batch_size=64,\n",
    "    epochs = 1000,\n",
    "    callbacks = [es],\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "725fa096-43ab-47d6-bc0b-3481f7e1163e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([16, 22], dtype=object),\n",
       "  array([285, 439], dtype=object),\n",
       "  array([29, 38], dtype=object),\n",
       "  array([50, 64], dtype=object),\n",
       "  array([56, 75], dtype=object)],\n",
       " array([[ 21.693808,  31.219471],\n",
       "        [350.7031  , 510.3231  ],\n",
       "        [127.51409 ,  59.4153  ],\n",
       "        [ 50.592693,  72.68108 ],\n",
       "        [ 78.906456, 103.257126]], dtype=float32))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_test[:5], y_pred[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcbddc8-7986-4213-9f0e-71e65aeca82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "i = random.randint(0,100)\n",
    "print(i)\n",
    "\n",
    "df = pd.DataFrame(X_test[i], columns=['open', 'high', 'low', 'close'])\n",
    "df = df[(df != -100).all(axis=1)]\n",
    "\n",
    "fig = go.Figure(data=[go.Candlestick(x=df.index,\n",
    "                open=df.open,\n",
    "                high=df.high,\n",
    "                low=df.low,\n",
    "                close=df.close)])\n",
    "    \n",
    "y_min, y_max = y_test[i]\n",
    "fig.add_vline(x=y_min, line_width=3, line_dash=\"dash\", line_color=\"green\")\n",
    "fig.add_vline(x=y_max, line_width=3, line_dash=\"dash\", line_color=\"green\")\n",
    "\n",
    "y_min, y_max = y_pred[i]\n",
    "fig.add_vline(x=y_min, line_width=3, line_dash=\"dash\", line_color=\"blue\")\n",
    "fig.add_vline(x=y_max, line_width=3, line_dash=\"dash\", line_color=\"blue\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "y_pred[i]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(y_train[:, 0], y_train[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "59abf372-c45c-458d-88f1-386df0b6cf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27, 36], dtype=object)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "35ddad3e-f646-45b0-9153-83dafc3c7585",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43my_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "y_test[i].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ab69d898-449e-482a-88c5-a9776baae4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 7ms/step - loss: 912.7903 - mae: 15.3244\n"
     ]
    }
   ],
   "source": [
    "y_test = [entry[:2] for entry in y_test]\n",
    "X_test = tf.convert_to_tensor(X_test, np.float32)\n",
    "y_test = tf.convert_to_tensor(y_test, np.int16)\n",
    "res = model.evaluate(X_test, y_test)\n",
    "\n",
    "results_dict['Double Bottom'] = res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf33a917-52bb-42d1-9428-ee2a2652b0ca",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bdb3d249-99aa-4db4-9f0f-17d26d359299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Results DF'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rising Wedge</th>\n",
       "      <th>Falling Wedge</th>\n",
       "      <th>Double Top</th>\n",
       "      <th>Double Bottom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.821136</td>\n",
       "      <td>41.822731</td>\n",
       "      <td>23.219362</td>\n",
       "      <td>15.324409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rising Wedge  Falling Wedge  Double Top  Double Bottom\n",
       "1     25.821136      41.822731   23.219362      15.324409"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results_dict, index=[1])\n",
    "display('Results DF')\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c6e04a-bca0-4882-b7e6-733952b1a338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
